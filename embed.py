#embed.py

"""
Documentation:

Dependencies:
Requires qdrant-client, sentence-transformers, dotenv, os, json, and argparse.

Key Functionality:
The script primarily focuses on reading text and writing vectors to a cloud-hosted or local Qdrant instance.

    1. Configuration and Loading
    Environment Variables: It loads QDRANT_URL and QDRANT_API_KEY from the .env file for secure access to the vector database.
    load_chunks_from_json(filepath): This utility function reads the list of text chunks, which are typically generated by the ingest.py script, from a specified JSON file.

    2. create_embeddings_and_upload(chunks, ...)
    Model Initialization: It initializes the SentenceTransformer model, specifically 'all-MiniLM-L6-v2', which is used to convert the text into numerical vectors.
    Qdrant Client Setup: It creates a connection to the Qdrant service using the environment variables.
    Collection Management: It uses client.recreate_collection() to ensure a fresh collection exists with the correct vector dimensions and a COSINE similarity metric for search.
    Embedding Generation: It processes the list of text chunks to generate their corresponding vector representations.
    Point Preparation: It creates a list of models.PointStruct objects, where each point contains:
    An id (the index of the chunk).
    The vector (the calculated embedding).
    A payload containing the original text, "chunk_text".
    Upload: It calls client.upsert() to batch upload the points (vectors and payloads) to the specified Qdrant collection, which defaults to "knowledge_base".

    3. Execution (main())
    It takes a JSON filename as a command-line argument using argparse.
    It loads the chunks and then initiates the embedding and upload process.
"""

import os
import sqlite3
import argparse
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

def load_claims_for_embedding(db_path="knowledge.db"):
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute("SELECT claim_id, claim_text, line_id, source_ref FROM claims")
    claims = [{"claim_id": row[0], "claim_text": row[1], "line_id": row[2], "source_ref": row[3]} for row in c.fetchall()]
    conn.close()
    return claims

def create_embeddings_and_upload_claims(claims, collection_name="knowledge_base"):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
    client.recreate_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(size=model.get_sentence_embedding_dimension(), distance=models.Distance.COSINE)
    )
    vectors = model.encode([claim["claim_text"]for claim in claims], show_progress_bar=True).tolist()
    points = [
        models.PointStruct(
            id=int(claim["claim_id"]),
            vector=vectors[i],
            payload={
                "claim_id": int(claim["claim_id"]),
                "claim_text": claim["claim_text"],
                "line_id": claim["line_id"],
                "source_ref": claim["source_ref"]
            }
        )
        for i, claim in enumerate(claims)
    ]
    client.upsert(collection_name=collection_name, wait=True, points=points)
    print(f"Uploaded {len(claims)} claims to Qdrant.")

def main ():
    """
    Main function to create embeddings for claims and upload to Qdrant.
    """
    claims = load_claims_for_embedding("knowledge.db")
    if claims:
        create_embeddings_and_upload_claims(claims)
    else:
        print("No claims found in the database.")

if __name__ == "__main__":
    main()